# 10/13/16

Chapters 2.1, 2.2, 2.3 to study for the midterm

## Nlog(N) Barrier

**Claim: No compare based sorting algorithm can guarantee to sort N items with fewer than Nlog(N) compares**

Proof:
```
Assume all keys are distinct
Define the notion of a compare tree
The Compare Tree is a binary tree with some additional rules:
	- Each leaf node represents a completed sort outcome for your array
	- Each internal node represents a compare operation on the pair (i,j) compare (a[i],a[j])
	- Left subtrees represent --> sequence of compares where a[i] was less than a[j]
	- Right subtrees represent --> sequence of compares where a[i] was greater than a[j]
	- From all of these assumptions we can say that each path from the root to a leaf node represents the sequence of compares to produce the outcome at the leaf node
	- we can see from the compare tree that there are N! ways of arranging these keys, this also means we have N! sort operation outcomes
	- we can see from the compare tree that the best case is the leaf on the far left of the tree and the worst case is the leaf on the far Right
	- The height of the tree is the worst case of the sort
	- if we have a binary tree of height h we know that it has no more that 2<sup>h</sup> nodes
	- Every compare tree has some a height h and some number of leaf nodes that are between N! and 2<sup>h</sup>

N! <= 2<sup>h</sup> and N! < l <= 2<sup>h</sup>

log(!N) <= log(2<sup>h</sup>)

log(N!) <= h

log(1)+.....log(N)

~ Nlog(N) <= h

~ Nlog(N) < l <= h

we have shown for this general case that we cannot do better than Nlog(N)

```

---

**A priori key knowledge**

Shannon Entropy: N keys with k distinct values
for each i from 1 to K
F<sub>i</sub> : frequency of key K<sub>i</sub>
P<sub>i</sub> : frequency of that key divided by the number of keys which is the probability of key Ki
H = Shannon entropy which is the negative sum of the keys from i = 1 to K P<sub>i</sub> * log(P<sub>i</sub>)
In the worst case when these are distinct this will collapse into an Nlog(N) time magnitude
In the best case when these are the same this will be of order 1 time magnitude


---

## Quick sort
lets build an algorithm that can take advantage of the non uniqueness of keys
Quick Sort uses a partitioning system to sort the elements

**Partition**
A partition re-arranges an array to enforce the following rules
	1. One value is a[j] is in its final place
	2. No entry from a[min] up to a[j-1] is greater than the value at a[j]
	3. No entry from a[j+1] to a[max] is less than the value at a[j]
	4. we call that value that is in its final place the pivot

```
A = {T I N Y C A T}
assume pivot is A[0]
where the resulting array is {C I N T T A Y}

- this is not a partitioning function because the value C is not at its final spot

```

```
A = {T I N Y C A T}
assume pivot is A[0]
where the resulting array is
{C I N A T T Y}

- this is a partition because the T is in its final spot and everything after the T is greater than it and everything before is is less than T
```

```
A = {T I N Y C A T}
assume pivot is A[3]
where the resulting array is
{T I N T C A Y}

- This partition is correct even though the pivot is in the far right spot.

```

Pseudo Code for Quick Sort

```
quicksort(A, lo, hi)
	input:	A - array
					lo - starting position
					hi - end position

	if lo < hi then
      p := partition(A, lo, hi)
      quicksort(A, lo, p – 1)
      quicksort(A, p + 1, hi)

partition(A, lo, hi)
  pivot := A[hi]
  i := lo        // place for swapping
  for j := lo to hi – 1 do
      if A[j] ≤ pivot then
          swap A[i] with A[j]
          i := i + 1
  swap A[i] with A[hi]
  return i
```
